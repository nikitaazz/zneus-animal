{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d61fa-0740-46bf-8388-9b9a415be1b3",
   "metadata": {
    "id": "341d61fa-0740-46bf-8388-9b9a415be1b3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb72797",
   "metadata": {},
   "source": [
    "### W&B setup (Colab-safe)\n",
    "\n",
    "- Set `WANDB_API_KEY` in the environment for automatic login.\n",
    "\n",
    "- If no key is present or login fails, we auto-switch to offline mode so runs still proceed.\n",
    "\n",
    "- You can also manually set `USE_WANDB = False` to skip logging entirely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d978177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W&B login via built-in prompt (asks for key if not set)\n",
    "USE_WANDB = True\n",
    "if USE_WANDB:\n",
    "    try:\n",
    "        wandb.login(relogin=True, force=True)\n",
    "        print(\"wandb: logged in via wandb.login prompt\")\n",
    "    except Exception as e:\n",
    "        print(f\"wandb login prompt failed ({e}); going offline\")\n",
    "        os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "        USE_WANDB = False\n",
    "else:\n",
    "    print(\"wandb disabled; skipping login prompt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3a5e4-2907-4495-8c1b-cd96400b831d",
   "metadata": {
    "id": "72e3a5e4-2907-4495-8c1b-cd96400b831d"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f52b584-87a6-4a11-a533-078c981e3a28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f52b584-87a6-4a11-a533-078c981e3a28",
    "outputId": "3f54fcb8-beaf-42fe-b395-773c79e42c09"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0079edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "print(f\"Seeds set to {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d94b4-8059-45d1-a940-4b0cbf6b79fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "784d94b4-8059-45d1-a940-4b0cbf6b79fd",
    "outputId": "e67c4e0d-37fb-45a5-84e9-4e000e9f5538"
   },
   "outputs": [],
   "source": [
    "path = Path(kagglehub.dataset_download(\"alessiocorrado99/animals10\")) / \"raw-img\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb542a2-4220-4eb6-badc-94c4a233b96d",
   "metadata": {
    "id": "9bb542a2-4220-4eb6-badc-94c4a233b96d"
   },
   "source": [
    "Animals type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea00acc-39df-4892-9cc5-e618eba5cf49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aea00acc-39df-4892-9cc5-e618eba5cf49",
    "outputId": "e3d38763-731b-4016-ac25-8915b01ddd91"
   },
   "outputs": [],
   "source": [
    "clases = os.listdir(path)\n",
    "print(clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd4985-b9f0-4984-9794-e5e8f6a03a4c",
   "metadata": {
    "id": "89dd4985-b9f0-4984-9794-e5e8f6a03a4c"
   },
   "source": [
    "Num of photos in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d48dc3-8bb4-4a45-a848-e7ac6fa632a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32d48dc3-8bb4-4a45-a848-e7ac6fa632a3",
    "outputId": "01c120bd-aefd-4e11-e27b-013da6f412eb"
   },
   "outputs": [],
   "source": [
    "print(f\"{'Class Name':<20} | {'Count'}\")\n",
    "print(\"-\" * 30)\n",
    "for cl in clases:\n",
    "    folder_path = path / cl\n",
    "    count = len(os.listdir(folder_path))\n",
    "    print(f\"{cl:<20} | {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c42f7-e9cd-4ffd-b4bc-7249dca7bae5",
   "metadata": {
    "id": "c55c42f7-e9cd-4ffd-b4bc-7249dca7bae5"
   },
   "source": [
    "See what we have in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69d8c7-9896-4303-98fa-ed4d16c5f3d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7b69d8c7-9896-4303-98fa-ed4d16c5f3d9",
    "outputId": "a84e4354-42e8-4c16-de7a-0c9fc6d7e18a"
   },
   "outputs": [],
   "source": [
    "for cl in clases:\n",
    "    n=3\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    a = path / cl\n",
    "    imgs = os.listdir(a)\n",
    "    i=1\n",
    "    for _ in range(n):\n",
    "        img_path = a / imgs[_]\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        plt.subplot(4, n, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca15a1e-1f08-452b-87d1-b00aae80a184",
   "metadata": {
    "id": "2ca15a1e-1f08-452b-87d1-b00aae80a184"
   },
   "source": [
    "Analyze avg size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16b73d-5f28-4b52-8e45-d7e24281b046",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a16b73d-5f28-4b52-8e45-d7e24281b046",
    "outputId": "7852b2d3-3c84-403e-a620-a0f3edb802ab"
   },
   "outputs": [],
   "source": [
    "print(f\"{'Class':<20} | {'Min W':<6} | {'Max W':<6} | {'Avg W':<8} | {'Min H':<6} | {'Max H':<6} | {'Avg H':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Ensure unique classes and sort them\n",
    "unique_clases = sorted(list(set(clases)))\n",
    "\n",
    "for cl in unique_clases:\n",
    "    folder_path = path / cl\n",
    "    widths = []\n",
    "    heights = []\n",
    "\n",
    "    # Skip if not a directory\n",
    "    if not folder_path.is_dir():\n",
    "        continue\n",
    "\n",
    "    # Get all images in the class folder\n",
    "    try:\n",
    "        imgs = os.listdir(folder_path)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    for im_name in imgs:\n",
    "        img_path = folder_path / im_name\n",
    "        try:\n",
    "            with Image.open(img_path) as im:\n",
    "                w, h = im.size\n",
    "                widths.append(w)\n",
    "                heights.append(h)\n",
    "        except Exception as e:\n",
    "            # print(f\"Error reading {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if widths and heights:\n",
    "        min_w = min(widths)\n",
    "        max_w = max(widths)\n",
    "        avg_w = sum(widths) / len(widths)\n",
    "\n",
    "        min_h = min(heights)\n",
    "        max_h = max(heights)\n",
    "        avg_h = sum(heights) / len(heights)\n",
    "\n",
    "        print(f\"{cl:<20} | {min_w:<6} | {max_w:<6} | {avg_w:<8.1f} | {min_h:<6} | {max_h:<6} | {avg_h:<8.1f}\")\n",
    "    else:\n",
    "        print(f\"{cl:<20} | No images found or error reading images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90effb69",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "90effb69",
    "outputId": "c79a1e9b-e2b8-4bca-d122-62d863ad1894"
   },
   "outputs": [],
   "source": [
    "# Analyze Image Channels\n",
    "modes = {}\n",
    "channels_count = {}\n",
    "\n",
    "print(\"Analyzing image modes and channels\")\n",
    "for cl in clases:\n",
    "    folder = path / cl\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = folder / img_name\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                # Count modes (RGB, L, CMYK, etc.)\n",
    "                modes[img.mode] = modes.get(img.mode, 0) + 1\n",
    "                # Count channels\n",
    "                c = len(img.getbands())\n",
    "                channels_count[c] = channels_count.get(c, 0) + 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"Image Modes:\", modes)\n",
    "print(\"Channel Counts:\", channels_count)\n",
    "\n",
    "if 1 in channels_count or 4 in channels_count:\n",
    "    print(\"\\nNote: Dataset contains images with different channel counts.\")\n",
    "    print(\"We will convert all images to RGB (3 channels/parameters per pixel) during preprocessing.\")\n",
    "else:\n",
    "    print(\"\\nAll images are already 3 channels (RGB).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d8f37",
   "metadata": {
    "id": "059d8f37"
   },
   "source": [
    "Analyze if RGB images are truly colorful or grayscale-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148f68f",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4148f68f",
    "outputId": "126c28b3-d2bd-48ed-d1ec-75b45e91cdaa"
   },
   "outputs": [],
   "source": [
    "# Analyze color distribution in images\n",
    "# Check if RGB images are truly colorful or essentially grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def analyze_color_info(img):\n",
    "    \"\"\"\n",
    "    Analyze if an image is truly colorful or grayscale-like.\n",
    "    Returns metrics about color saturation and channel differences.\n",
    "    \"\"\"\n",
    "    img_array = np.array(img.convert(\"RGB\"))\n",
    "\n",
    "    # Calculate channel-wise statistics\n",
    "    r, g, b = img_array[:,:,0], img_array[:,:,1], img_array[:,:,2]\n",
    "\n",
    "    # Check if channels are similar (grayscale-like)\n",
    "    # If R ≈ G ≈ B for all pixels, image is grayscale\n",
    "    rg_diff = np.abs(r.astype(float) - g.astype(float)).mean()\n",
    "    rb_diff = np.abs(r.astype(float) - b.astype(float)).mean()\n",
    "    gb_diff = np.abs(g.astype(float) - b.astype(float)).mean()\n",
    "\n",
    "    avg_channel_diff = (rg_diff + rb_diff + gb_diff) / 3\n",
    "\n",
    "    # Calculate color saturation using HSV-like approach\n",
    "    max_channel = np.maximum(np.maximum(r, g), b).astype(float)\n",
    "    min_channel = np.minimum(np.minimum(r, g), b).astype(float)\n",
    "\n",
    "    # Saturation: difference between max and min channel\n",
    "    saturation = np.where(max_channel > 0, (max_channel - min_channel) / max_channel, 0)\n",
    "    avg_saturation = saturation.mean()\n",
    "\n",
    "    return avg_channel_diff, avg_saturation\n",
    "\n",
    "print(\"Analyzing color characteristics per class...\")\n",
    "print(f\"{'Class':<20} | {'Colorful':<10} | {'Grayscale-like':<15} | {'Avg Channel Diff':<18} | {'Avg Saturation':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "unique_clases = sorted(list(set(clases)))\n",
    "\n",
    "for cl in unique_clases:\n",
    "    folder_path = path / cl\n",
    "\n",
    "    if not folder_path.is_dir():\n",
    "        continue\n",
    "\n",
    "    colorful_count = 0\n",
    "    grayscale_like_count = 0\n",
    "    total_channel_diff = 0\n",
    "    total_saturation = 0\n",
    "    valid_images = 0\n",
    "\n",
    "    imgs = os.listdir(folder_path)\n",
    "\n",
    "    for im_name in imgs:\n",
    "        img_path = folder_path / im_name\n",
    "        try:\n",
    "            with Image.open(img_path) as im:\n",
    "                channel_diff, saturation = analyze_color_info(im)\n",
    "                total_channel_diff += channel_diff\n",
    "                total_saturation += saturation\n",
    "                valid_images += 1\n",
    "\n",
    "                # Threshold: if avg channel difference < 5, consider grayscale-like\n",
    "                if channel_diff < 5:\n",
    "                    grayscale_like_count += 1\n",
    "                else:\n",
    "                    colorful_count += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if valid_images > 0:\n",
    "        avg_diff = total_channel_diff / valid_images\n",
    "        avg_sat = total_saturation / valid_images\n",
    "        print(f\"{cl:<20} | {colorful_count:<10} | {grayscale_like_count:<15} | {avg_diff:<18.2f} | {avg_sat:<15.4f}\")\n",
    "\n",
    "print(\"\\nNote: Grayscale-like = RGB images where R ≈ G ≈ B (avg channel diff < 5)\")\n",
    "print(\"Saturation: 0 = no color, 1 = fully saturated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292f99f-84ba-4759-835e-0635b04558cb",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "d292f99f-84ba-4759-835e-0635b04558cb",
    "outputId": "c04c1938-d06b-42f1-e0e6-9ab9b2b2f020"
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "labels = []\n",
    "class_names = sorted(os.listdir(path))\n",
    "\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "for cl in clases:\n",
    "    folder = path / cl\n",
    "    # Verify folder exists\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    for img in os.listdir(folder):\n",
    "        files.append(str(folder / img))\n",
    "        labels.append(class_to_idx[cl])\n",
    "\n",
    "print(f\"Total images loaded: {len(files)}\")\n",
    "print(f\"Total classes: {len(class_names)}\")\n",
    "print(f\"Class mapping: {class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7b4a1-ed23-415b-b44e-c0113665fd26",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3fe7b4a1-ed23-415b-b44e-c0113665fd26",
    "outputId": "8c556a0c-8092-4bfe-e8e5-33394a2c5487"
   },
   "outputs": [],
   "source": [
    "print(\"Splitting dataset into Train / Val / Test sets...\")\n",
    "train_files, temp_files, train_labels, temp_labels = train_test_split(\n",
    "    files, labels,\n",
    "    test_size=0.30,\n",
    "    stratify=labels,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    " )\n",
    " \n",
    "# Split remaining 30% into validation and test (15% / 15%)\n",
    "val_files, test_files, val_labels, test_labels = train_test_split(\n",
    "    temp_files, temp_labels,\n",
    "    test_size=0.50,\n",
    "    stratify=temp_labels,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    " )\n",
    " \n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n",
    "print(f\"Testing samples: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b502c5-c55c-4853-810a-a1c700d74236",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "39b502c5-c55c-4853-810a-a1c700d74236"
   },
   "outputs": [],
   "source": [
    "#Budeme vyuzivat accuracy a f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d8564-1a85-4272-b499-2f5ad4f8193d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "IS_COLAB = \"COLAB_GPU\" in os.environ\n",
    "\n",
    "\n",
    "MAX_WORKERS = os.cpu_count() or 2\n",
    "\n",
    "\n",
    "ALLOW_MULTIPROC = os.environ.get(\"ALLOW_MULTIPROC_DATALOADER\", \"0\") == \"1\"\n",
    "\n",
    "\n",
    "# Colab warns above 2 workers; keep it small there. Otherwise allow up to 4 on GPU.\n",
    "\n",
    "if IS_COLAB:\n",
    "\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "elif device.type == \"cuda\":\n",
    "\n",
    "    NUM_WORKERS = min(4, MAX_WORKERS)\n",
    "\n",
    "else:\n",
    "\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "\n",
    "# Notebook-safe default: force single-process loading to avoid worker shutdown AssertionError.\n",
    "\n",
    "if not ALLOW_MULTIPROC:\n",
    "\n",
    "    NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "\n",
    "PIN_MEMORY = device.type == \"cuda\" and NUM_WORKERS > 0\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "\n",
    "    transforms.Resize((170, 170)),\n",
    "\n",
    "    transforms.ToTensor()\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "dataset = datasets.ImageFolder(path, transform=transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "\n",
    "    dataset,\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    num_workers=NUM_WORKERS,\n",
    "\n",
    "    pin_memory=PIN_MEMORY,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "sum_ = torch.zeros(3, dtype=torch.double)\n",
    "\n",
    "sum_sq = torch.zeros(3, dtype=torch.double)\n",
    "\n",
    "total_pixels = 0\n",
    "\n",
    "\n",
    "\n",
    "for images, _ in loader:\n",
    "\n",
    "    b, c, h, w = images.shape\n",
    "\n",
    "    num_pixels_in_batch = b * h * w\n",
    "\n",
    "\n",
    "\n",
    "    sum_ += images.sum(dim=[0, 2, 3]).double()\n",
    "\n",
    "    sum_sq += (images ** 2).sum(dim=[0, 2, 3]).double()\n",
    "\n",
    "    total_pixels += num_pixels_in_batch\n",
    "\n",
    "\n",
    "\n",
    "mean_calculated = sum_ / total_pixels\n",
    "\n",
    "var = (sum_sq / total_pixels) - (mean_calculated ** 2)\n",
    "\n",
    "std_calculated = torch.sqrt(var)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Mean:\", mean_calculated.tolist())\n",
    "\n",
    "print(\"Std: \", std_calculated.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe98d9-9386-444c-a044-d0ee77761591",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "abbe98d9-9386-444c-a044-d0ee77761591",
    "outputId": "97db952a-19ed-4807-ee60-cdb7e19080c5"
   },
   "outputs": [],
   "source": [
    "# Define training transformations with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert(\"RGB\")), # Ensure 3 channels (R, G, B)\n",
    "    transforms.Resize((170, 170)),                 # Resize to consistent size\n",
    "    transforms.RandomHorizontalFlip(p=0.5),        # Data Augmentation: Flip\n",
    "    #transforms.RandomRotation(5),                  # Data Augmentation: Rotate\n",
    "    transforms.ColorJitter(                        # Data Augmentation: Color\n",
    "        brightness=0.15,\n",
    "        contrast=0.15\n",
    "    ),\n",
    "    transforms.ToTensor(),                         # Convert to Tensor (0-1 range)\n",
    "    transforms.Normalize(                          # Normalize to range [-1, 1]\n",
    "        # mean = [0.517707626799793, 0.5003007536238897, 0.4125557296095215],\n",
    "        # std = [0.26413124327703913, 0.25914410756442957, 0.27683583555842795]\n",
    "        mean = mean_calculated.tolist(),\n",
    "        std = std_calculated.tolist()\n",
    "\n",
    "    )\n",
    "])\n",
    "print(\"Training transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ee99c",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b06ee99c",
    "outputId": "8b8c96b5-cc12-4cf0-e505-4281906867ef"
   },
   "outputs": [],
   "source": [
    "# Visualization of Augmentations\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def visualize_augmentations(files, transform, n_samples=3):\n",
    "    \"\"\"\n",
    "    Visualizes original images and their augmented versions.\n",
    "    \"\"\"\n",
    "    # Select random files\n",
    "    sample_files = random.sample(files, n_samples)\n",
    "\n",
    "    for img_path in sample_files:\n",
    "        # Load original\n",
    "        original_img = Image.open(img_path)\n",
    "\n",
    "        # Apply transform multiple times to see variations\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "        # Show original\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        mean = np.array(mean_calculated.tolist())\n",
    "        std = np.array(std_calculated.tolist())\n",
    "\n",
    "        for i in range(1, 5):\n",
    "            # Apply transform\n",
    "            aug_tensor = transform(original_img)\n",
    "\n",
    "            # Un-normalize and convert to numpy for display\n",
    "            aug_img = aug_tensor.permute(1, 2, 0).numpy()\n",
    "            aug_img = (aug_img * std) + mean\n",
    "            aug_img = np.clip(aug_img, 0, 1)\n",
    "\n",
    "            axes[i].imshow(aug_img)\n",
    "            axes[i].set_title(f\"Aug {i}\")\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print(\"Visualizing Training Augmentations:\")\n",
    "visualize_augmentations(train_files, train_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e2e6c-6b28-44f8-b219-292a0727c330",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "647e2e6c-6b28-44f8-b219-292a0727c330",
    "outputId": "fc32f041-3f8d-436f-bf90-bf578c1a3ff5"
   },
   "outputs": [],
   "source": [
    "# Define test transformations (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert(\"RGB\")), # Ensure 3 channels\n",
    "    transforms.Resize((170, 170)),                 # Resize to consistent size\n",
    "    transforms.ToTensor(),                         # Convert to Tensor\n",
    "    transforms.Normalize(                          # Normalize to range [-1, 1]\n",
    "        mean_calculated,\n",
    "        std_calculated\n",
    "    )\n",
    "])\n",
    "print(\"Test transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8b3ca",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "82f8b3ca",
    "outputId": "ca591a4e-fd8f-4828-e670-2dc6e7c35ae1"
   },
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Initialize Datasets\n",
    "print(\"Initializing Datasets...\")\n",
    "train_dataset = AnimalDataset(train_files, train_labels, transform=train_transform)\n",
    "val_dataset = AnimalDataset(val_files, val_labels, transform=test_transform)\n",
    "test_dataset = AnimalDataset(test_files, test_labels, transform=test_transform)\n",
    "\n",
    "# Initialize DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "print(f\"Initializing DataLoaders with batch size {BATCH_SIZE} and {NUM_WORKERS} workers...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=PIN_MEMORY,\n",
    ")\n",
    "\n",
    "print(\"Data preparation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea8faa",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1aea8faa",
    "outputId": "0b8afce0-595d-45b6-f364-93ae9ff10d4f"
   },
   "outputs": [],
   "source": [
    "# Visualize Test Data\n",
    "print(\"Visualizing samples from Test Dataset (Preprocessed)...\")\n",
    "\n",
    "def visualize_dataset_samples(dataset, class_names, n_samples=5):\n",
    "    indices = random.sample(range(len(dataset)), n_samples)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_samples, figsize=(15, 3))\n",
    "\n",
    "    mean = np.array(mean_calculated.tolist())\n",
    "    std = np.array(std_calculated.tolist())\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, label = dataset[idx]\n",
    "\n",
    "        # Denormalize for visualization\n",
    "        img_display = image.permute(1, 2, 0).numpy()\n",
    "        img_display = (img_display * std) + mean\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "        axes[i].imshow(img_display)\n",
    "        axes[i].set_title(class_names[label])\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_dataset_samples(test_dataset, class_names, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef800e",
   "metadata": {
    "id": "e6ef800e"
   },
   "source": [
    "# CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b148b1f-5cc4-4fa4-897d-b3538f20ca7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # =============================\n",
    "    # Dataset and preprocessing\n",
    "    # =============================\n",
    "    \"data\": {\n",
    "        \"num_classes\": 10,\n",
    "        \"input_size\": (170, 170),\n",
    "        # \"normalize_mean\": [0.517707626799793, 0.5003007536238897, 0.4125557296095215],\n",
    "        # \"normalize_std\":  [0.26413124327703913, 0.25914410756442957, 0.27683583555842795],\n",
    "        \"normalize_mean\": mean_calculated.tolist(),\n",
    "        \"normalize_std\": std_calculated.tolist(),\n",
    "        \"class_names\": ['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
    "    },\n",
    "\n",
    "    # =============================\n",
    "    # Augmentations\n",
    "    # =============================\n",
    "    \"augmentations\": {\n",
    "        \"train\": {\n",
    "            \"horizontal_flip\": True,\n",
    "            \"vertical_flip\": False,\n",
    "            \"color_jitter\": True,\n",
    "        },\n",
    "        \"val\": {\n",
    "            \"resize\": False,\n",
    "            \"center_crop\": True,\n",
    "        }\n",
    "    },\n",
    "\n",
    "    # =============================\n",
    "    # Model Architecture\n",
    "    # =============================\n",
    "    \"model\": {\n",
    "        \"name\": \"cnn\",\n",
    "\n",
    "        # Convolution blocks settings\n",
    "        \"conv_blocks\": [\n",
    "            {\"out_channels\": 32, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "            {\"out_channels\": 64, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "            {\"out_channels\": 128, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "        ],\n",
    "\n",
    "        # Architecture options\n",
    "        \"use_batchnorm\": True,\n",
    "        \"use_dropout\": True,\n",
    "        \"dropout_rate\": 0.3,\n",
    "\n",
    "        # Residual & bottleneck features\n",
    "        \"use_skip_connections\": True,\n",
    "        \"use_bottleneck\": False,\n",
    "\n",
    "        # Fully connected head\n",
    "        \"fc_layers\": [256, 64],  # e.g. 128 -> 256 -> 64 -> 10 classes\n",
    "    },\n",
    "\n",
    "    # =============================\n",
    "    # Training\n",
    "    # =============================\n",
    "    \"train\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 40,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"scheduler\": \"cosine\",\n",
    "        \"num_workers\": 4,\n",
    "        \"pin_memory\": True,\n",
    "        \"seed\": 42,\n",
    "    },\n",
    "\n",
    "    # =============================\n",
    "    # Experiment tracking\n",
    "    # =============================\n",
    "    \"experiment\": {\n",
    "        \"name\": \"animals_resnet18_baseline\",\n",
    "        \"description\": \"Baseline animal classification with augmentations\",\n",
    "        \"save_best_only\": True,\n",
    "        \"metric\": \"accuracy\",\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c655f4-b950-40c0-b0b0-d62dc242a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "        k = cfg.get(\"kernel\", 3)\n",
    "        s = cfg.get(\"stride\", 1)\n",
    "        p = cfg.get(\"padding\", 1)\n",
    "\n",
    "        use_bn = cfg.get(\"use_batchnorm\", False)\n",
    "        use_do = cfg.get(\"use_dropout\", False)\n",
    "        dr = cfg.get(\"dropout_rate\", 0.3)\n",
    "        bottleneck = cfg.get(\"use_bottleneck\", False)\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        if bottleneck:\n",
    "            mid = out_ch // 4\n",
    "\n",
    "            layers += [\n",
    "                nn.Conv2d(in_ch, mid, kernel_size=1),\n",
    "                nn.BatchNorm2d(mid) if use_bn else nn.Identity(),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(mid, mid, kernel_size=k, stride=s, padding=p),\n",
    "                nn.BatchNorm2d(mid) if use_bn else nn.Identity(),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(mid, out_ch, kernel_size=1),\n",
    "                nn.BatchNorm2d(out_ch) if use_bn else nn.Identity(),\n",
    "            ]\n",
    "        else:\n",
    "\n",
    "            layers += [\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=k, stride=s, padding=p),\n",
    "                nn.BatchNorm2d(out_ch) if use_bn else nn.Identity(),\n",
    "                nn.ReLU(),\n",
    "            ]\n",
    "\n",
    "        if use_do:\n",
    "            layers += [nn.Dropout2d(dr)]\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "        needs_proj = (in_ch != out_ch) or (s != 1)\n",
    "        self.proj = (\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=s)\n",
    "            if needs_proj\n",
    "            else nn.Identity()\n",
    "        )\n",
    "        self.use_skip = cfg.get(\"use_skip_connections\", False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if self.use_skip:\n",
    "            out += self.proj(x)\n",
    "        return F.relu(out)\n",
    "    \n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, cfg: dict):\n",
    "        super().__init__()\n",
    "\n",
    "        model_cfg = cfg[\"model\"]\n",
    "        conv_cfgs = model_cfg[\"conv_blocks\"]\n",
    "\n",
    "        use_bn = model_cfg.get(\"use_batchnorm\", False)\n",
    "        use_do = model_cfg.get(\"use_dropout\", False)\n",
    "        dr = model_cfg.get(\"dropout_rate\", 0.3)\n",
    "        use_skip = model_cfg.get(\"use_skip_connections\", False)\n",
    "        use_bottleneck = model_cfg.get(\"use_bottleneck\", False)\n",
    "\n",
    "        fc_layers = model_cfg[\"fc_layers\"]\n",
    "        num_classes = cfg[\"data\"][\"num_classes\"]\n",
    "\n",
    "\n",
    "\n",
    "        conv_blocks = []\n",
    "        in_ch = 3\n",
    "\n",
    "        for block in conv_cfgs:\n",
    "            block_cfg = {\n",
    "                **block,\n",
    "                \"use_batchnorm\": use_bn,\n",
    "                \"use_dropout\": use_do,\n",
    "                \"dropout_rate\": dr,\n",
    "                \"use_skip_connections\": use_skip,\n",
    "                \"use_bottleneck\": use_bottleneck,\n",
    "            }\n",
    "\n",
    "            conv_blocks.append(ConvBlock(in_ch, block[\"out_channels\"], block_cfg))\n",
    "            in_ch = block[\"out_channels\"]\n",
    "\n",
    "        self.conv = nn.Sequential(*conv_blocks)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        layers = []\n",
    "        in_features = in_ch\n",
    "\n",
    "        for hidden in fc_layers:\n",
    "            layers += [\n",
    "                nn.Linear(in_features, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dr) if use_do else nn.Identity(),\n",
    "            ]\n",
    "            in_features = hidden\n",
    "\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        self.fc = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1441d-d648-4103-a8bb-5cbb2c2d8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN(config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc2782",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cedc2782",
    "outputId": "af3ddd77-ef8c-40a9-f5fa-c8b78e28c5a6"
   },
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(\"Loss function: CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e0ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configurations\n",
    "base_data_cfg = {\n",
    "    \"num_classes\": len(class_names),\n",
    "    \"input_size\": (170, 170),\n",
    "    \"normalize_mean\": mean_calculated.tolist(),\n",
    "    \"normalize_std\": std_calculated.tolist(),\n",
    "    \"class_names\": class_names,\n",
    "}\n",
    "\n",
    "experiments = {\n",
    "    \"exp1_baseline\": {\n",
    "        \"description\": \"Shallow baseline CNN for quick feedback\",\n",
    "        \"model\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"out_channels\": 32, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "                {\"out_channels\": 64, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "            ],\n",
    "            \"use_batchnorm\": True,\n",
    "            \"use_dropout\": True,\n",
    "            \"dropout_rate\": 0.2,\n",
    "            \"use_skip_connections\": False,\n",
    "            \"use_bottleneck\": False,\n",
    "            \"fc_layers\": [128],\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"epochs\": 300,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"early_stop_patience\": 30,\n",
    "            \"monitor_metric\": \"accuracy\",\n",
    "        },\n",
    "    },\n",
    "    \"exp2_deeper\": {\n",
    "        \"description\": \"Deeper CNN with higher capacity\",\n",
    "        \"model\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"out_channels\": 32, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "                {\"out_channels\": 64, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "                {\"out_channels\": 128, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "                {\"out_channels\": 256, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "            ],\n",
    "            \"use_batchnorm\": True,\n",
    "            \"use_dropout\": True,\n",
    "            \"dropout_rate\": 0.3,\n",
    "            \"use_skip_connections\": False,\n",
    "            \"use_bottleneck\": False,\n",
    "            \"fc_layers\": [256, 128],\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"epochs\": 300,\n",
    "            \"learning_rate\": 8e-4,\n",
    "            \"weight_decay\": 1e-4,\n",
    "            \"early_stop_patience\": 30,\n",
    "            \"monitor_metric\": \"accuracy\",\n",
    "        },\n",
    "    },\n",
    "    \"exp3_skip_dropout\": {\n",
    "        \"description\": \"Skip connections + dropout/batchnorm\",\n",
    "        \"model\": {\n",
    "            \"conv_blocks\": [\n",
    "                {\"out_channels\": 32, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "                {\"out_channels\": 64, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "                {\"out_channels\": 128, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "                {\"out_channels\": 128, \"kernel\": 3, \"stride\": 1, \"padding\": 1},\n",
    "            ],\n",
    "            \"use_batchnorm\": True,\n",
    "            \"use_dropout\": True,\n",
    "            \"dropout_rate\": 0.4,\n",
    "            \"use_skip_connections\": True,\n",
    "            \"use_bottleneck\": False,\n",
    "            \"fc_layers\": [256, 64],\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"epochs\": 300,\n",
    "            \"learning_rate\": 7e-4,\n",
    "            \"weight_decay\": 2e-4,\n",
    "            \"early_stop_patience\": 30,\n",
    "            \"monitor_metric\": \"accuracy\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "print(\"Experiment configurations defined (3 setups).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a503d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f40a503d",
    "outputId": "1de13252-8677-4dc3-f30f-bd41232918d1"
   },
   "outputs": [],
   "source": [
    "# Metrics helpers\n",
    "def _compute_prf1(labels, preds):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"precision\": precision * 100,\n",
    "        \"recall\": recall * 100,\n",
    "        \"f1\": f1 * 100,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Train\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += batch_size\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = total_loss / max(total, 1)\n",
    "    epoch_acc = 100 * correct / max(total, 1)\n",
    "    metrics = {\n",
    "        \"loss\": epoch_loss,\n",
    "        \"accuracy\": epoch_acc,\n",
    "        **_compute_prf1(all_labels, all_preds),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def validate(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=\"Val\", leave=False)\n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += batch_size\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = total_loss / max(total, 1)\n",
    "    epoch_acc = 100 * correct / max(total, 1)\n",
    "    metrics = {\n",
    "        \"loss\": epoch_loss,\n",
    "        \"accuracy\": epoch_acc,\n",
    "        **_compute_prf1(all_labels, all_preds),\n",
    "    }\n",
    "    return metrics, all_preds, all_labels\n",
    "\n",
    "print(\"Training and validation functions defined with tqdm and metrics.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26e0ef",
   "metadata": {
    "id": "ba26e0ef"
   },
   "outputs": [],
   "source": [
    "# Experiment runner\n",
    "def build_model_from_experiment(exp_cfg):\n",
    "    cfg = {\n",
    "        \"model\": exp_cfg[\"model\"],\n",
    "        \"data\": base_data_cfg,\n",
    "    }\n",
    "    return CustomCNN(cfg).to(device)\n",
    "\n",
    "\n",
    "def train_and_evaluate(exp_name, exp_cfg, use_wandb=True):\n",
    "    epochs = exp_cfg[\"train\"][\"epochs\"]\n",
    "    lr = exp_cfg[\"train\"][\"learning_rate\"]\n",
    "    wd = exp_cfg[\"train\"][\"weight_decay\"]\n",
    "    patience = exp_cfg[\"train\"].get(\"early_stop_patience\")\n",
    "    monitor_metric = exp_cfg[\"train\"].get(\"monitor_metric\", \"accuracy\")\n",
    "\n",
    "    model = build_model_from_experiment(exp_cfg)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", factor=0.5, patience=2\n",
    "    )\n",
    "\n",
    "    history = {\"train\": [], \"val\": []}\n",
    "    best_metric = float(\"-inf\")\n",
    "    best_model_path = f\"best_model_{exp_name}.pth\"\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    run = None\n",
    "    if use_wandb:\n",
    "        run = wandb.init(\n",
    "            project=\"zneus-animal\",\n",
    "            name=exp_name,\n",
    "            reinit=True,\n",
    "            config={\n",
    "                \"epochs\": epochs,\n",
    "                \"learning_rate\": lr,\n",
    "                \"weight_decay\": wd,\n",
    "                \"model\": exp_cfg[\"model\"],\n",
    "                \"description\": exp_cfg.get(\"description\", \"\"),\n",
    "                \"monitor_metric\": monitor_metric,\n",
    "                \"early_stop_patience\": patience,\n",
    "            },\n",
    "        )\n",
    "\n",
    "    print(f\"\\n=== Starting {exp_name} ({exp_cfg.get('description','')}) ===\")\n",
    "    for epoch in range(epochs):\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_metrics, _, _ = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        scheduler.step(val_metrics[\"loss\"])\n",
    "\n",
    "        history[\"train\"].append(train_metrics)\n",
    "        history[\"val\"].append(val_metrics)\n",
    "\n",
    "        metric_value = val_metrics.get(monitor_metric, float(\"-inf\"))\n",
    "        improved = metric_value > best_metric\n",
    "        if improved:\n",
    "            best_metric = metric_value\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        log_payload = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train/loss\": train_metrics[\"loss\"],\n",
    "            \"train/accuracy\": train_metrics[\"accuracy\"],\n",
    "            \"train/precision\": train_metrics[\"precision\"],\n",
    "            \"train/recall\": train_metrics[\"recall\"],\n",
    "            \"train/f1\": train_metrics[\"f1\"],\n",
    "            \"val/loss\": val_metrics[\"loss\"],\n",
    "            \"val/accuracy\": val_metrics[\"accuracy\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "            \"val/early_stop_metric\": metric_value,\n",
    "        }\n",
    "        if run:\n",
    "            run.log(log_payload)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "            f\"Train Loss: {train_metrics['loss']:.4f} | Train Acc: {train_metrics['accuracy']:.2f}% | \"\n",
    "            f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.2f}% | \"\n",
    "            f\"Val F1: {val_metrics['f1']:.2f}%\"\n",
    "        )\n",
    "\n",
    "        if patience is not None and epochs_no_improve >= patience:\n",
    "            print(\n",
    "                f\"Early stopping triggered after {epoch+1} epochs (no {monitor_metric} improvement for {patience} epochs).\"\n",
    "            )\n",
    "            if run:\n",
    "                run.log({\"early_stopped_epoch\": epoch + 1})\n",
    "            break\n",
    "\n",
    "    # Reload best model and evaluate on test\n",
    "    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    test_metrics, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "\n",
    "    if run:\n",
    "        run.log({\n",
    "            \"test/loss\": test_metrics[\"loss\"],\n",
    "            \"test/accuracy\": test_metrics[\"accuracy\"],\n",
    "            \"test/precision\": test_metrics[\"precision\"],\n",
    "            \"test/recall\": test_metrics[\"recall\"],\n",
    "            \"test/f1\": test_metrics[\"f1\"],\n",
    "            \"best_val_metric\": best_metric,\n",
    "        })\n",
    "        try:\n",
    "            run.log({\n",
    "                \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    preds=test_preds,\n",
    "                    y_true=test_labels,\n",
    "                    class_names=class_names,\n",
    "                )\n",
    "            })\n",
    "        except Exception:\n",
    "            pass\n",
    "        run.finish()\n",
    "\n",
    "    print(\n",
    "        f\"\\n>>> {exp_name} done | Best Val {monitor_metric}: {best_metric:.2f} | \"\n",
    "        f\"Test Acc: {test_metrics['accuracy']:.2f}% | Test F1: {test_metrics['f1']:.2f}%\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"history\": history,\n",
    "        \"best_val_metric\": best_metric,\n",
    "        \"test_metrics\": test_metrics,\n",
    "        \"test_preds\": test_preds,\n",
    "        \"test_labels\": test_labels,\n",
    "        \"best_model_path\": best_model_path,\n",
    "    }\n",
    "\n",
    "# Run experiments\n",
    "use_wandb = globals().get(\"USE_WANDB\", True)\n",
    "experiment_results = {}\n",
    "for exp_name, exp_cfg in experiments.items():\n",
    "    experiment_results[exp_name] = train_and_evaluate(exp_name, exp_cfg, use_wandb=use_wandb)\n",
    "print(\"All experiments completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd504a",
   "metadata": {
    "id": "febd504a"
   },
   "outputs": [],
   "source": [
    "# Plot training history across experiments (solid=train, dashed=val per experiment)\n",
    "metrics_to_plot = [\"loss\", \"accuracy\", \"f1\"]\n",
    "color_map = plt.cm.get_cmap(\"tab10\")\n",
    "exp_names = list(experiment_results.keys())\n",
    "colors = {name: color_map(i % 10) for i, name in enumerate(exp_names)}\n",
    "\n",
    "for metric in metrics_to_plot:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    for exp_name, res in experiment_results.items():\n",
    "        color = colors[exp_name]\n",
    "        train_curve = [m[metric] for m in res[\"history\"][\"train\"]]\n",
    "        val_curve = [m[metric] for m in res[\"history\"][\"val\"]]\n",
    "        epochs_range = range(1, len(train_curve) + 1)\n",
    "        plt.plot(epochs_range, train_curve, label=f\"{exp_name} train\", color=color, linestyle=\"-\")\n",
    "        plt.plot(epochs_range, val_curve, label=f\"{exp_name} val\", color=color, linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    ylabel = 'Value' if metric == 'loss' else f\"{metric.capitalize()} (%)\"\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(f\"{metric.capitalize()} curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Tabular summary of test metrics\n",
    "rows = []\n",
    "for exp_name, res in experiment_results.items():\n",
    "    tm = res[\"test_metrics\"]\n",
    "    rows.append({\n",
    "        \"experiment\": exp_name,\n",
    "        \"test_loss\": tm[\"loss\"],\n",
    "        \"test_acc\": tm[\"accuracy\"],\n",
    "        \"test_precision\": tm[\"precision\"],\n",
    "        \"test_recall\": tm[\"recall\"],\n",
    "        \"test_f1\": tm[\"f1\"],\n",
    "        \"best_val_metric\": res.get(\"best_val_metric\"),\n",
    "    })\n",
    "summary_df = pd.DataFrame(rows)\n",
    "display(summary_df.sort_values(by=\"test_f1\", ascending=False).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3011fc",
   "metadata": {
    "id": "fc3011fc"
   },
   "outputs": [],
   "source": [
    "# Confusion matrices per experiment\n",
    "import seaborn as sns\n",
    "\n",
    "for exp_name, res in experiment_results.items():\n",
    "    preds = res[\"test_preds\"]\n",
    "    labels = res[\"test_labels\"]\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'Confusion Matrix - {exp_name}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
