{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d61fa-0740-46bf-8388-9b9a415be1b3",
   "metadata": {
    "id": "341d61fa-0740-46bf-8388-9b9a415be1b3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch\n",
    "import kagglehub\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e3a5e4-2907-4495-8c1b-cd96400b831d",
   "metadata": {
    "id": "72e3a5e4-2907-4495-8c1b-cd96400b831d"
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f52b584-87a6-4a11-a533-078c981e3a28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7f52b584-87a6-4a11-a533-078c981e3a28",
    "outputId": "3f54fcb8-beaf-42fe-b395-773c79e42c09"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d94b4-8059-45d1-a940-4b0cbf6b79fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "784d94b4-8059-45d1-a940-4b0cbf6b79fd",
    "outputId": "e67c4e0d-37fb-45a5-84e9-4e000e9f5538"
   },
   "outputs": [],
   "source": [
    "path = Path(kagglehub.dataset_download(\"alessiocorrado99/animals10\")) / \"raw-img\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb542a2-4220-4eb6-badc-94c4a233b96d",
   "metadata": {
    "id": "9bb542a2-4220-4eb6-badc-94c4a233b96d"
   },
   "source": [
    "Animals type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea00acc-39df-4892-9cc5-e618eba5cf49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aea00acc-39df-4892-9cc5-e618eba5cf49",
    "outputId": "e3d38763-731b-4016-ac25-8915b01ddd91"
   },
   "outputs": [],
   "source": [
    "clases = os.listdir(path)\n",
    "print(clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dd4985-b9f0-4984-9794-e5e8f6a03a4c",
   "metadata": {
    "id": "89dd4985-b9f0-4984-9794-e5e8f6a03a4c"
   },
   "source": [
    "Num of photos in each folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d48dc3-8bb4-4a45-a848-e7ac6fa632a3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32d48dc3-8bb4-4a45-a848-e7ac6fa632a3",
    "outputId": "01c120bd-aefd-4e11-e27b-013da6f412eb"
   },
   "outputs": [],
   "source": [
    "print(f\"{'Class Name':<20} | {'Count'}\")\n",
    "print(\"-\" * 30)\n",
    "for cl in clases:\n",
    "    folder_path = path / cl\n",
    "    count = len(os.listdir(folder_path))\n",
    "    print(f\"{cl:<20} | {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c42f7-e9cd-4ffd-b4bc-7249dca7bae5",
   "metadata": {
    "id": "c55c42f7-e9cd-4ffd-b4bc-7249dca7bae5"
   },
   "source": [
    "See what we have in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b69d8c7-9896-4303-98fa-ed4d16c5f3d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7b69d8c7-9896-4303-98fa-ed4d16c5f3d9",
    "outputId": "a84e4354-42e8-4c16-de7a-0c9fc6d7e18a"
   },
   "outputs": [],
   "source": [
    "for cl in clases:\n",
    "    n=3\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    a = path / cl\n",
    "    imgs = os.listdir(a)\n",
    "    i=1\n",
    "    for _ in range(n):\n",
    "        img_path = a / imgs[_]\n",
    "        img = Image.open(img_path)\n",
    "\n",
    "        plt.subplot(4, n, i)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca15a1e-1f08-452b-87d1-b00aae80a184",
   "metadata": {
    "id": "2ca15a1e-1f08-452b-87d1-b00aae80a184"
   },
   "source": [
    "Analyze avg size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a16b73d-5f28-4b52-8e45-d7e24281b046",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a16b73d-5f28-4b52-8e45-d7e24281b046",
    "outputId": "7852b2d3-3c84-403e-a620-a0f3edb802ab"
   },
   "outputs": [],
   "source": [
    "print(f\"{'Class':<20} | {'Min W':<6} | {'Max W':<6} | {'Avg W':<8} | {'Min H':<6} | {'Max H':<6} | {'Avg H':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Ensure unique classes and sort them\n",
    "unique_clases = sorted(list(set(clases)))\n",
    "\n",
    "for cl in unique_clases:\n",
    "    folder_path = path / cl\n",
    "    widths = []\n",
    "    heights = []\n",
    "\n",
    "    # Skip if not a directory\n",
    "    if not folder_path.is_dir():\n",
    "        continue\n",
    "\n",
    "    # Get all images in the class folder\n",
    "    try:\n",
    "        imgs = os.listdir(folder_path)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    for im_name in imgs:\n",
    "        img_path = folder_path / im_name\n",
    "        try:\n",
    "            with Image.open(img_path) as im:\n",
    "                w, h = im.size\n",
    "                widths.append(w)\n",
    "                heights.append(h)\n",
    "        except Exception as e:\n",
    "            # print(f\"Error reading {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if widths and heights:\n",
    "        min_w = min(widths)\n",
    "        max_w = max(widths)\n",
    "        avg_w = sum(widths) / len(widths)\n",
    "\n",
    "        min_h = min(heights)\n",
    "        max_h = max(heights)\n",
    "        avg_h = sum(heights) / len(heights)\n",
    "\n",
    "        print(f\"{cl:<20} | {min_w:<6} | {max_w:<6} | {avg_w:<8.1f} | {min_h:<6} | {max_h:<6} | {avg_h:<8.1f}\")\n",
    "    else:\n",
    "        print(f\"{cl:<20} | No images found or error reading images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90effb69",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "90effb69",
    "outputId": "c79a1e9b-e2b8-4bca-d122-62d863ad1894"
   },
   "outputs": [],
   "source": [
    "# Analyze Image Channels\n",
    "modes = {}\n",
    "channels_count = {}\n",
    "\n",
    "print(\"Analyzing image modes and channels\")\n",
    "for cl in clases:\n",
    "    folder = path / cl\n",
    "    for img_name in os.listdir(folder):\n",
    "        img_path = folder / img_name\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                # Count modes (RGB, L, CMYK, etc.)\n",
    "                modes[img.mode] = modes.get(img.mode, 0) + 1\n",
    "                # Count channels\n",
    "                c = len(img.getbands())\n",
    "                channels_count[c] = channels_count.get(c, 0) + 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "print(\"Image Modes:\", modes)\n",
    "print(\"Channel Counts:\", channels_count)\n",
    "\n",
    "if 1 in channels_count or 4 in channels_count:\n",
    "    print(\"\\nNote: Dataset contains images with different channel counts.\")\n",
    "    print(\"We will convert all images to RGB (3 channels/parameters per pixel) during preprocessing.\")\n",
    "else:\n",
    "    print(\"\\nAll images are already 3 channels (RGB).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d8f37",
   "metadata": {
    "id": "059d8f37"
   },
   "source": [
    "Analyze if RGB images are truly colorful or grayscale-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148f68f",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "4148f68f",
    "outputId": "126c28b3-d2bd-48ed-d1ec-75b45e91cdaa"
   },
   "outputs": [],
   "source": [
    "# Analyze color distribution in images\n",
    "# Check if RGB images are truly colorful or essentially grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def analyze_color_info(img):\n",
    "    \"\"\"\n",
    "    Analyze if an image is truly colorful or grayscale-like.\n",
    "    Returns metrics about color saturation and channel differences.\n",
    "    \"\"\"\n",
    "    img_array = np.array(img.convert(\"RGB\"))\n",
    "\n",
    "    # Calculate channel-wise statistics\n",
    "    r, g, b = img_array[:,:,0], img_array[:,:,1], img_array[:,:,2]\n",
    "\n",
    "    # Check if channels are similar (grayscale-like)\n",
    "    # If R ≈ G ≈ B for all pixels, image is grayscale\n",
    "    rg_diff = np.abs(r.astype(float) - g.astype(float)).mean()\n",
    "    rb_diff = np.abs(r.astype(float) - b.astype(float)).mean()\n",
    "    gb_diff = np.abs(g.astype(float) - b.astype(float)).mean()\n",
    "\n",
    "    avg_channel_diff = (rg_diff + rb_diff + gb_diff) / 3\n",
    "\n",
    "    # Calculate color saturation using HSV-like approach\n",
    "    max_channel = np.maximum(np.maximum(r, g), b).astype(float)\n",
    "    min_channel = np.minimum(np.minimum(r, g), b).astype(float)\n",
    "\n",
    "    # Saturation: difference between max and min channel\n",
    "    saturation = np.where(max_channel > 0, (max_channel - min_channel) / max_channel, 0)\n",
    "    avg_saturation = saturation.mean()\n",
    "\n",
    "    return avg_channel_diff, avg_saturation\n",
    "\n",
    "print(\"Analyzing color characteristics per class...\")\n",
    "print(f\"{'Class':<20} | {'Colorful':<10} | {'Grayscale-like':<15} | {'Avg Channel Diff':<18} | {'Avg Saturation':<15}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "unique_clases = sorted(list(set(clases)))\n",
    "\n",
    "for cl in unique_clases:\n",
    "    folder_path = path / cl\n",
    "\n",
    "    if not folder_path.is_dir():\n",
    "        continue\n",
    "\n",
    "    colorful_count = 0\n",
    "    grayscale_like_count = 0\n",
    "    total_channel_diff = 0\n",
    "    total_saturation = 0\n",
    "    valid_images = 0\n",
    "\n",
    "    imgs = os.listdir(folder_path)\n",
    "\n",
    "    for im_name in imgs:\n",
    "        img_path = folder_path / im_name\n",
    "        try:\n",
    "            with Image.open(img_path) as im:\n",
    "                channel_diff, saturation = analyze_color_info(im)\n",
    "                total_channel_diff += channel_diff\n",
    "                total_saturation += saturation\n",
    "                valid_images += 1\n",
    "\n",
    "                # Threshold: if avg channel difference < 5, consider grayscale-like\n",
    "                if channel_diff < 5:\n",
    "                    grayscale_like_count += 1\n",
    "                else:\n",
    "                    colorful_count += 1\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if valid_images > 0:\n",
    "        avg_diff = total_channel_diff / valid_images\n",
    "        avg_sat = total_saturation / valid_images\n",
    "        print(f\"{cl:<20} | {colorful_count:<10} | {grayscale_like_count:<15} | {avg_diff:<18.2f} | {avg_sat:<15.4f}\")\n",
    "\n",
    "print(\"\\nNote: Grayscale-like = RGB images where R ≈ G ≈ B (avg channel diff < 5)\")\n",
    "print(\"Saturation: 0 = no color, 1 = fully saturated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d292f99f-84ba-4759-835e-0635b04558cb",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "d292f99f-84ba-4759-835e-0635b04558cb",
    "outputId": "c04c1938-d06b-42f1-e0e6-9ab9b2b2f020"
   },
   "outputs": [],
   "source": [
    "files = []\n",
    "labels = []\n",
    "class_names = sorted(os.listdir(path))\n",
    "\n",
    "class_to_idx = {c: i for i, c in enumerate(class_names)}\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "for cl in clases:\n",
    "    folder = path / cl\n",
    "    # Verify folder exists\n",
    "    if not folder.is_dir():\n",
    "        continue\n",
    "\n",
    "    for img in os.listdir(folder):\n",
    "        files.append(str(folder / img))\n",
    "        labels.append(class_to_idx[cl])\n",
    "\n",
    "print(f\"Total images loaded: {len(files)}\")\n",
    "print(f\"Total classes: {len(class_names)}\")\n",
    "print(f\"Class mapping: {class_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7b4a1-ed23-415b-b44e-c0113665fd26",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "3fe7b4a1-ed23-415b-b44e-c0113665fd26",
    "outputId": "8c556a0c-8092-4bfe-e8e5-33394a2c5487"
   },
   "outputs": [],
   "source": [
    "print(\"Splitting dataset into Train and Test sets...\")\n",
    "train_files, test_files, train_labels, test_labels = train_test_split(\n",
    "    files, labels,\n",
    "    test_size=0.2,\n",
    "    stratify=labels,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Testing samples: {len(test_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b502c5-c55c-4853-810a-a1c700d74236",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "39b502c5-c55c-4853-810a-a1c700d74236"
   },
   "outputs": [],
   "source": [
    "#Budeme vyuzivat accuracy a f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe98d9-9386-444c-a044-d0ee77761591",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "abbe98d9-9386-444c-a044-d0ee77761591",
    "outputId": "97db952a-19ed-4807-ee60-cdb7e19080c5"
   },
   "outputs": [],
   "source": [
    "# Define training transformations with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert(\"RGB\")), # Ensure 3 channels (R, G, B)\n",
    "    transforms.Resize((170, 170)),                 # Resize to consistent size\n",
    "    transforms.RandomHorizontalFlip(p=0.5),        # Data Augmentation: Flip\n",
    "    #transforms.RandomRotation(5),                  # Data Augmentation: Rotate\n",
    "    transforms.ColorJitter(                        # Data Augmentation: Color\n",
    "        brightness=0.15,\n",
    "        contrast=0.15\n",
    "    ),\n",
    "    transforms.ToTensor(),                         # Convert to Tensor (0-1 range)\n",
    "    transforms.Normalize(                          # Normalize to range [-1, 1]\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "print(\"Training transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ee99c",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "b06ee99c",
    "outputId": "8b8c96b5-cc12-4cf0-e505-4281906867ef"
   },
   "outputs": [],
   "source": [
    "# Visualization of Augmentations\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def visualize_augmentations(files, transform, n_samples=3):\n",
    "    \"\"\"\n",
    "    Visualizes original images and their augmented versions.\n",
    "    \"\"\"\n",
    "    # Select random files\n",
    "    sample_files = random.sample(files, n_samples)\n",
    "\n",
    "    for img_path in sample_files:\n",
    "        # Load original\n",
    "        original_img = Image.open(img_path)\n",
    "\n",
    "        # Apply transform multiple times to see variations\n",
    "        fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "\n",
    "        # Show original\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis('off')\n",
    "\n",
    "        for i in range(1, 5):\n",
    "            # Apply transform\n",
    "            aug_tensor = transform(original_img)\n",
    "\n",
    "            # Un-normalize and convert to numpy for display\n",
    "            # Using simple 0.5 mean/std\n",
    "            mean = np.array([0.5, 0.5, 0.5])\n",
    "            std = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "            aug_img = aug_tensor.permute(1, 2, 0).numpy()\n",
    "            aug_img = std * aug_img + mean\n",
    "            aug_img = np.clip(aug_img, 0, 1)\n",
    "\n",
    "            axes[i].imshow(aug_img)\n",
    "            axes[i].set_title(f\"Aug {i}\")\n",
    "            axes[i].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "print(\"Visualizing Training Augmentations:\")\n",
    "visualize_augmentations(train_files, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e2e6c-6b28-44f8-b219-292a0727c330",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "647e2e6c-6b28-44f8-b219-292a0727c330",
    "outputId": "fc32f041-3f8d-436f-bf90-bf578c1a3ff5"
   },
   "outputs": [],
   "source": [
    "# Define test transformations (no augmentation)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.convert(\"RGB\")), # Ensure 3 channels\n",
    "    transforms.Resize((170, 170)),                 # Resize to consistent size\n",
    "    transforms.ToTensor(),                         # Convert to Tensor\n",
    "    transforms.Normalize(                          # Normalize to range [-1, 1]\n",
    "        mean=[0.5, 0.5, 0.5],\n",
    "        std=[0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "print(\"Test transforms defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8b3ca",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "82f8b3ca",
    "outputId": "ca591a4e-fd8f-4828-e670-2dc6e7c35ae1"
   },
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.file_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Initialize Datasets\n",
    "print(\"Initializing Datasets...\")\n",
    "train_dataset = AnimalDataset(train_files, train_labels, transform=train_transform)\n",
    "test_dataset = AnimalDataset(test_files, test_labels, transform=test_transform)\n",
    "\n",
    "# Initialize DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "print(f\"Initializing DataLoaders with batch size {BATCH_SIZE}...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"Data preparation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea8faa",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1aea8faa",
    "outputId": "0b8afce0-595d-45b6-f364-93ae9ff10d4f"
   },
   "outputs": [],
   "source": [
    "# Visualize Test Data\n",
    "print(\"Visualizing samples from Test Dataset (Preprocessed)...\")\n",
    "\n",
    "def visualize_dataset_samples(dataset, class_names, n_samples=5):\n",
    "    indices = random.sample(range(len(dataset)), n_samples)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_samples, figsize=(15, 3))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        image, label = dataset[idx]\n",
    "\n",
    "        # Denormalize for visualization\n",
    "        mean = np.array([0.5, 0.5, 0.5])\n",
    "        std = np.array([0.5, 0.5, 0.5])\n",
    "\n",
    "        img_display = image.permute(1, 2, 0).numpy()\n",
    "        img_display = std * img_display + mean\n",
    "        img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "        axes[i].imshow(img_display)\n",
    "        axes[i].set_title(class_names[label])\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_dataset_samples(test_dataset, class_names, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ef800e",
   "metadata": {
    "id": "e6ef800e"
   },
   "source": [
    "# CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71753aff",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "71753aff",
    "outputId": "60046940-a12b-4e17-ade6-752d39eb0545"
   },
   "outputs": [],
   "source": [
    "class AnimalCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AnimalCNN, self).__init__()\n",
    "\n",
    "        # Convolutional layers\n",
    "        # Input: 3 x 170 x 170\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)    # -> 32 x 170 x 170\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)                             # -> 32 x 85 x 85\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)   # -> 64 x 85 x 85\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)                             # -> 64 x 42 x 42\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # -> 128 x 42 x 42\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)                             # -> 128 x 21 x 21\n",
    "\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # -> 256 x 21 x 21\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)                             # -> 256 x 10 x 10\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(256 * 10 * 10, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Conv block 1\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "\n",
    "        # Conv block 2\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "\n",
    "        # Conv block 3\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Conv block 4\n",
    "        x = self.pool4(F.relu(self.bn4(self.conv4(x))))\n",
    "\n",
    "        # Fully connected\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(class_names)\n",
    "model = AnimalCNN(num_classes=num_classes).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(f\"Model created with {num_classes} output classes\")\n",
    "print(f\"Model moved to: {device}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc2782",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "cedc2782",
    "outputId": "af3ddd77-ef8c-40a9-f5fa-c8b78e28c5a6"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler - reduce LR when loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3\n",
    ")\n",
    "\n",
    "print(\"Loss function: CrossEntropyLoss\")\n",
    "print(\"Optimizer: Adam (lr=0.001)\")\n",
    "print(\"Scheduler: ReduceLROnPlateau (factor=0.5, patience=3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40a503d",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f40a503d",
    "outputId": "1de13252-8677-4dc3-f30f-bd41232918d1"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Validation function\n",
    "def validate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(test_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    return epoch_loss, epoch_acc, all_preds, all_labels\n",
    "\n",
    "print(\"Training and validation functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba26e0ef",
   "metadata": {
    "id": "ba26e0ef"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = validate(model, test_loader, criterion, device)\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Training complete! Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd504a",
   "metadata": {
    "id": "febd504a"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(range(1, NUM_EPOCHS+1), train_losses, 'b-', label='Train Loss')\n",
    "axes[0].plot(range(1, NUM_EPOCHS+1), val_losses, 'r-', label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "axes[1].plot(range(1, NUM_EPOCHS+1), train_accs, 'b-', label='Train Acc')\n",
    "axes[1].plot(range(1, NUM_EPOCHS+1), val_accs, 'r-', label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3011fc",
   "metadata": {
    "id": "fc3011fc"
   },
   "outputs": [],
   "source": [
    "# Final evaluation with metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "\n",
    "# Get predictions\n",
    "val_loss, val_acc, all_preds, all_labels = validate(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"Final Test Accuracy: {val_acc:.2f}%\\n\")\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
